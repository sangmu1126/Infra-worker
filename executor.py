import os
import shutil
import time
import shutil
import time
import zipfile
import json
import structlog
import boto3
import docker
from pathlib import Path
from collections import deque
from dataclasses import dataclass, field
from typing import List, Optional, Dict
from datetime import datetime
from uploader import OutputUploader

logger = structlog.get_logger()

# --- Data Models ---
@dataclass
class TaskMessage:
    request_id: str
    function_id: str
    runtime: str
    s3_key: str
    s3_key: str
    s3_bucket: Optional[str] = None
    memory_mb: int = 128
    timeout_ms: int = 300000
    payload: Dict = field(default_factory=dict)

@dataclass
class ExecutionResult:
    request_id: str
    success: bool
    exit_code: int
    stdout: str
    stderr: str
    duration_ms: int
    peak_memory_bytes: Optional[int] = None
    allocated_memory_mb: Optional[int] = None
    optimization_tip: Optional[str] = None
    estimated_savings: Optional[str] = None
    output_files: List[str] = field(default_factory=list)

    def to_dict(self):
        return {
            "requestId": self.request_id,
            "status": "SUCCESS" if self.success else "FAILED",
            "exitCode": self.exit_code,
            "stdout": self.stdout,
            "stderr": self.stderr,
            "durationMs": self.duration_ms,
            "peakMemoryBytes": self.peak_memory_bytes,
            "allocatedMemoryMb": self.allocated_memory_mb,
            "optimizationTip": self.optimization_tip,
            "estimatedSavings": self.estimated_savings,
            "outputFiles": self.output_files
        }

# --- Service Logic ---

class AutoTuner:
    """ë©”ëª¨ë¦¬ ìµœì í™” íŒ ìƒì„± (ë¹„ìš© ì ˆê° í•µì‹¬)"""
    COST_PER_MB_HOUR = 0.00005  # $0.00005 (ì„ì˜ì˜ AWS EC2 GB-hour ë¹„ìš© ê¸°ë°˜ ì¶”ì •)

    @staticmethod
    def analyze(peak_bytes: int, allocated_mb: int):
        if not peak_bytes: return None, None
        if allocated_mb <= 0: allocated_mb = 128  # 0 ë‚˜ëˆ„ê¸° ë°©ì–´

        peak_mb = peak_bytes / (1024 * 1024)
        
        # 1. íŒ ìƒì„±
        tip = None
        ratio = peak_mb / allocated_mb
        if ratio < 0.3:
            rec = max(int(peak_mb * 1.5), 10) # ìµœì†Œ 10MB ê¶Œì¥
            saved_percent = int((1 - (rec / allocated_mb)) * 100)
            if saved_percent > 0:
                tip = f"ğŸ’¡ Tip: ì‹¤ì œ ì‚¬ìš©ëŸ‰({int(peak_mb)}MB)ì´ í• ë‹¹ëŸ‰({allocated_mb}MB)ë³´ë‹¤ í›¨ì”¬ ì ìŠµë‹ˆë‹¤. {rec}MBë¡œ ì¤„ì—¬ ë¹„ìš©ì„ ì•½ {saved_percent}% ì ˆê°í•˜ì„¸ìš”."
        elif ratio > 0.9:
            rec = int(peak_mb * 1.2)
            tip = f"âš ï¸ Warning: ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({int(peak_mb)}MB). {rec}MB ì´ìƒìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."

        # 2. ë¹„ìš© ì ˆê°ì•¡ ê³„ì‚° (ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì )
        # ê°€ì •: ê¸°ì¡´ VM ì˜¤ë²„í—¤ë“œ 1024MB ëŒ€ë¹„ ì ˆê°
        vm_overhead_mb = 1024
        saved_mb = vm_overhead_mb - peak_mb
        estimated_savings = None
        
        if saved_mb > 0:
            # ì›”ê°„ ì ˆê°ì•¡ (730ì‹œê°„ ê¸°ì¤€)
            monthly_saving = saved_mb * AutoTuner.COST_PER_MB_HOUR * 730
            estimated_savings = f"${monthly_saving:.2f}/month (vs 1GB VM)"

        return tip, estimated_savings

class CloudWatchPublisher:
    """ASG ì—°ë™ì„ ìœ„í•œ CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡"""
    def __init__(self, region):
        self.client = boto3.client("cloudwatch", region_name=region)
        
    def publish_peak_memory(self, func_id, runtime, bytes_used):
        try:
            if bytes_used is None: return
            # ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ë©´ ë” ì¢‹ìŒ (ì—¬ê¸°ì„  ë¡œê¹…ë§Œ)
            # logger.debug("Publishing CloudWatch Metric", value=bytes_used)
            self.client.put_metric_data(
                Namespace="NanoGrid/FunctionRunner",
                MetricData=[{
                    "MetricName": "PeakMemoryBytes",
                    "Dimensions": [{"Name": "FunctionId", "Value": func_id}, {"Name": "Runtime", "Value": runtime}],
                    "Value": float(bytes_used),
                    "Unit": "Bytes",
                    "Timestamp": datetime.utcnow()
                }]
            )
        except Exception as e:
            logger.warning("CloudWatch publish failed", error=str(e))

class TaskExecutor:
    """í†µí•© ì‹¤í–‰ ì—”ì§„: S3 ë‹¤ìš´ë¡œë“œ -> Docker ì‹¤í–‰ -> ê²°ê³¼ ì²˜ë¦¬"""
    
    def __init__(self, config: Dict):
        self.cfg = config
        self.docker = docker.from_env()
        self.s3 = boto3.client("s3", region_name=config.get("AWS_REGION", "ap-northeast-2"))
        self.cw = CloudWatchPublisher(config.get("AWS_REGION", "ap-northeast-2"))
        self.uploader = OutputUploader(
            bucket_name=config.get("S3_USER_DATA_BUCKET", ""),
            region=config.get("AWS_REGION", "ap-northeast-2")
        )
        
        # Warm Pool ì €ì¥ì†Œ
        self.pools = {
            "python": deque(), "cpp": deque(), "nodejs": deque(), "go": deque()
        }
        self.images = {
            "python": config.get("DOCKER_PYTHON_IMAGE", "nanogrid/python:3.9-fat"),
            "cpp": config.get("DOCKER_CPP_IMAGE", "gcc:latest"),
            "nodejs": config.get("DOCKER_NODEJS_IMAGE", "node:18-alpine"),
            "go": config.get("DOCKER_GO_IMAGE", "golang:1.19-alpine")
        }
        
        self._initialize_warm_pool()

    def _initialize_warm_pool(self):
        """Warm Pool ì´ˆê¸°í™” (Cold Start ì œê±°)"""
        counts = {
            "python": int(self.cfg.get("WARM_POOL_PYTHON_SIZE", 1)),
            "cpp": int(self.cfg.get("WARM_POOL_CPP_SIZE", 1)),
            "nodejs": int(self.cfg.get("WARM_POOL_NODEJS_SIZE", 1)),
            "go": int(self.cfg.get("WARM_POOL_GO_SIZE", 1))
        }
        logger.info("ğŸ”¥ Initializing Warm Pools", counts=counts)
        
        for runtime, count in counts.items():
            for _ in range(count):
                self._create_warm_container(runtime)

    def _create_warm_container(self, runtime: str) -> str:
        try:
            img = self.images.get(runtime)
            # ë¬´í•œ ëŒ€ê¸° ì»¨í…Œì´ë„ˆ ì‹¤í–‰
            c = self.docker.containers.run(
                img, command="tail -f /dev/null", detach=True,
                # í˜¸ìŠ¤íŠ¸ ê²½ë¡œ ë§ˆìš´íŠ¸ (ì½”ë“œ ì‹¤í–‰ìš©) - ì½ê¸° ì „ìš©ìœ¼ë¡œ ë§ˆìš´íŠ¸í•˜ê±°ë‚˜ í•„ìš”í•œ ê²½ë¡œë§Œ ë§ˆìš´íŠ¸ ê¶Œì¥
                # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì „ì²´ ì‘ì—… ë£¨íŠ¸ë¥¼ ë§ˆìš´íŠ¸
                volumes={self.cfg["DOCKER_WORK_DIR_ROOT"]: {"bind": "/workspace", "mode": "rw"}},
                network_mode="bridge", # AI Endpoint ì ‘ê·¼ í—ˆìš©
                mem_limit=f"{task.memory_mb}m",    # ì»¨í…Œì´ë„ˆ í•˜ë“œ ë¦¬ë°‹ (Dynamic)
                cpu_quota=50000      # 0.5 CPU
            )
            c.pause()
            self.pools[runtime].append(c.id)
            return c.id
        except Exception as e:
            logger.error("Failed to create warm container", runtime=runtime, error=str(e))
            return None

    def _acquire_container(self, runtime: str):
        """Warm Poolì—ì„œ ì»¨í…Œì´ë„ˆ íšë“ (Unpause)"""
        target_runtime = runtime if runtime in self.pools else "python"
        
        # Poolì´ ë¹„ì–´ìˆìœ¼ë©´ ì¦‰ì‹œ ìƒì„± (ë™ê¸°)
        if not self.pools[target_runtime]:
            logger.warning("Pool empty, creating new container synchronously", runtime=target_runtime)
            cid = self._create_warm_container(target_runtime)
            # ë°©ê¸ˆ ë§Œë“ ê±´ append ë˜ì—ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ pop í•´ì•¼ í•¨
            if not cid: raise RuntimeError("Failed to create container")
            
        try:
            cid = self.pools[target_runtime].popleft()
            c = self.docker.containers.get(cid)
            if c.status == 'paused':
                c.unpause()
            return c
        except Exception:
            # ì‹¤íŒ¨ ì‹œ(ì´ë¯¸ ì£½ì€ ì»¨í…Œì´ë„ˆ ë“±) ì¬ê·€ì ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
            return self._acquire_container(target_runtime)

    def _prepare_workspace(self, task: TaskMessage) -> Path:
        """S3 ë‹¤ìš´ë¡œë“œ ë° [ë³´ì•ˆ] Zip Slip ë°©ì§€ ì••ì¶• í•´ì œ"""
        local_dir = Path(self.cfg["DOCKER_WORK_DIR_ROOT"]) / task.request_id
        if local_dir.exists(): shutil.rmtree(local_dir)
        local_dir.mkdir(parents=True, exist_ok=True)
        
        zip_path = local_dir / "code.zip"
        bucket = task.s3_bucket if task.s3_bucket else self.cfg["S3_CODE_BUCKET"]
        self.s3.download_file(bucket, task.s3_key, str(zip_path))
        
        # âœ… [FIX] Zip Slip ë°©ì§€ ì½”ë“œ ì ìš©
        with zipfile.ZipFile(zip_path, "r") as zf:
            for member in zf.namelist():
                # ìƒìœ„ ë””ë ‰í„°ë¦¬(../) ì ‘ê·¼ ì‹œë„ ì°¨ë‹¨
                target_path = (local_dir / member).resolve()
                if not str(target_path).startswith(str(local_dir.resolve())):
                    logger.warning("Zip Slip attempt detected", file=member)
                    continue
                
                # íŒŒì¼ ì¶”ì¶œ
                if member.endswith('/'):
                    target_path.mkdir(parents=True, exist_ok=True)
                else:
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    with zf.open(member) as source, open(target_path, "wb") as dest:
                        shutil.copyfileobj(source, dest)
        
        zip_path.unlink()
        return local_dir

    def run(self, task: TaskMessage) -> ExecutionResult:
        container = None
        host_work_dir = None
        start_time = time.time()
        
        try:
            # 1. ì‘ì—… ê³µê°„ ì¤€ë¹„
            host_work_dir = self._prepare_workspace(task)
            # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ (/workspace ê°€ ë°”ì¸ë“œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ ê·¸ í•˜ìœ„ ê²½ë¡œ ì‚¬ìš©)
            container_work_dir = f"/workspace/{task.request_id}"

            # 2. ì»¨í…Œì´ë„ˆ íšë“ (Warm Start)
            container = self._acquire_container(task.runtime)
            
            # 3. ì‹¤í–‰ ì»¤ë§¨ë“œ êµ¬ì„±
            # Output Directory Setup
            host_output_dir = host_work_dir / "output"
            host_output_dir.mkdir(parents=True, exist_ok=True)
            
            # Symlink /workspace/{req_id}/output -> /output inside container
            # This allows user code to write to /output transparently
            # Note: /output exists from Dockerfile, so we must remove it to create the symlink at /output
            setup_cmd = f"rm -rf /output && ln -s {container_work_dir}/output /output"

            # Environment Variables
            env_vars = {
                "PAYLOAD": json.dumps(task.payload),
                "AI_ENDPOINT": self.cfg.get("AI_ENDPOINT", "http://10.0.20.100:11434"),
                "JOB_ID": task.request_id
            }

            cmd = []
            if task.runtime == "python": 
                cmd = ["sh", "-c", f"{setup_cmd} && python {container_work_dir}/main.py"]
            elif task.runtime == "cpp":  
                # C++ì€ ì»´íŒŒì¼ í›„ ì‹¤í–‰
                cmd = ["sh", "-c", f"{setup_cmd} && g++ {container_work_dir}/main.cpp -o {container_work_dir}/out && {container_work_dir}/out"]
            elif task.runtime == "nodejs": 
                cmd = ["sh", "-c", f"{setup_cmd} && node {container_work_dir}/index.js"]
            elif task.runtime == "go":
                # GoëŠ” ë¹Œë“œ í›„ ì‹¤í–‰
                cmd = ["sh", "-c", f"{setup_cmd} && cd {container_work_dir} && go build -o main main.go && ./main"]

            # 4. ì‹¤í–‰ (Exec)
            logger.info("Exec command", cmd=cmd, container=container.id[:12])
            exit_code, output = container.exec_run(
                cmd, workdir="/workspace", demux=False,
                environment=env_vars
            )
            
            # âœ… [FIX] í•˜ë“œì½”ë”© ì œê±° & ì‹¤ì œ ë©”ëª¨ë¦¬ ì¸¡ì •
            try:
                stats = container.stats(stream=False)
                # Max usage is more accurate for peak memory during execution
                usage = stats['memory_stats'].get('max_usage', 0)
                # Fallback to usage if max_usage is 0 or missing (rare in normal docker)
                if usage == 0:
                    usage = stats['memory_stats'].get('usage', 0)
            except Exception as e:
                logger.warning("Failed to get metrics", error=str(e))
                usage = 0
            
            # 6. Auto-Tuning & CloudWatch
            tip, savings = AutoTuner.analyze(usage, task.memory_mb)
            self.cw.publish_peak_memory(task.function_id, task.runtime, usage)
            
            # 7. Output Upload
            # ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ /outputì— ì“´ íŒŒì¼ë“¤ì€ host_output_dirì— ì €ì¥ë¨
            output_files = self.uploader.upload_outputs(task.request_id, str(host_output_dir))

            output_str = output.decode('utf-8', errors='replace')

            return ExecutionResult(
                request_id=task.request_id,
                success=(exit_code == 0),
                exit_code=exit_code,
                stdout=output_str,
                stderr="",
                duration_ms=int((time.time() - start_time) * 1000),
                peak_memory_bytes=usage,
                allocated_memory_mb=task.memory_mb,
                optimization_tip=tip,
                estimated_savings=savings,
                output_files=output_files
            )

        except Exception as e:
            logger.error("Execution failed", error=str(e))
            return ExecutionResult(
                request_id=task.request_id, success=False, exit_code=-1,
                stdout="", stderr=str(e), duration_ms=int((time.time() - start_time) * 1000)
            )
            
        finally:
            # âœ… [FIX] ì˜¤ì—¼ëœ ì»¨í…Œì´ë„ˆëŠ” ì¬ì‚¬ìš©í•˜ì§€ ì•Šê³  íê¸°
            if container:
                try:
                    # Dirty Container Removal
                    container.remove(force=True)
                except: pass
            
            # íŒŒì¼ ì‚­ì œ
            if host_work_dir and host_work_dir.exists():
                try: shutil.rmtree(host_work_dir)
                except: pass