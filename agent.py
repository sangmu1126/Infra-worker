import os
import json
import time
import signal
import sys
import boto3
import redis
import structlog
from dotenv import load_dotenv
from prometheus_client import start_http_server, Counter, Histogram, Gauge

from executor import TaskExecutor, TaskMessage

# --- Setup ---
load_dotenv()

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="%Y-%m-%d %H:%M:%S"),
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.PrintLoggerFactory(),
)
logger = structlog.get_logger()

class NanoAgent:
    def __init__(self):
        logger.info("ğŸ¤– NanoGrid Agent Starting...")
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        self.config = {k: v for k, v in os.environ.items()}
        
        # Clients
        self.sqs = boto3.client('sqs', region_name=self.config.get("AWS_REGION", "ap-northeast-2"))
        self.redis_client = redis.Redis(
            host=self.config["REDIS_HOST"],
            port=int(self.config.get("REDIS_PORT", 6379)),
            decode_responses=True
        )
        
        # ì‹¤í–‰ ì—”ì§„ (Warm Pool í¬í•¨)
        self.executor = TaskExecutor(self.config)
        self.running = True

        # Prometheus Metrics
        self.jobs_processed = Counter('worker_jobs_processed_total', 'Total jobs processed', ['status', 'runtime'])
        self.job_duration = Histogram('worker_job_duration_seconds', 'Job execution duration in seconds', ['runtime'])
        self.active_jobs = Gauge('worker_active_jobs', 'Number of jobs currently running')

        signal.signal(signal.SIGINT, self._stop)
        signal.signal(signal.SIGTERM, self._stop)

    def _stop(self, signum, frame):
        logger.info("ğŸ›‘ Shutdown signal received")
        self.running = False

    def run(self):
        queue_url = self.config["SQS_QUEUE_URL"]
        logger.info("ğŸ“¡ Listening for tasks", queue=queue_url)

        # Start Metrics Server
        try:
            start_http_server(8000)
            logger.info("Prometheus Metrics Server Started", port=8000)
        except Exception as e:
            logger.error("Failed to start metrics server", error=str(e))

        while self.running:
            try:
                # 1. SQS Long Polling
                resp = self.sqs.receive_message(
                    QueueUrl=queue_url,
                    MaxNumberOfMessages=1,
                    WaitTimeSeconds=20
                )

                if "Messages" not in resp:
                    continue

                for msg in resp["Messages"]:
                    self._process_message(queue_url, msg)

            except Exception as e:
                logger.error("Polling loop error", error=str(e))
                time.sleep(1)
        
        logger.info("ğŸ‘‹ Agent stopped cleanly")

    def _process_message(self, queue_url, msg):
        task = None # Initialize task to None for error handling
        try:
            self.active_jobs.inc() # Increment active jobs gauge
            body = json.loads(msg["Body"])
            task = TaskMessage(
                request_id=body["requestId"],
                function_id=body.get("functionId", "unknown"),
                runtime=body.get("runtime", "python"),
                s3_key=body["s3Key"],
                s3_bucket=body.get("s3Bucket"),
                memory_mb=body.get("memoryMb", 128),
                timeout_ms=body.get("timeoutMs", 300000),
                payload=body.get("input", {})
            )
            
            logger.info("ğŸš€ Processing Task", id=task.request_id, runtime=task.runtime)

            # 2. ì‘ì—… ì‹¤í–‰ (Warm Pool ì‚¬ìš©)
            result = self.executor.run(task)

            # 3. ê²°ê³¼ Redis ë°œí–‰ (Pub/Sub + KV ì €ì¥)
            result_dict = result.to_dict()
            json_result = json.dumps(result_dict)
            
            # Pub/Sub ì±„ë„
            channel = f"result:{task.request_id}"
            self.redis_client.publish(channel, json_result)
            
            # Async ì¡°íšŒìš© í‚¤ ì €ì¥ (TTL 1ì‹œê°„)
            self.redis_client.setex(f"job:{task.request_id}", 3600, json_result)
            
            # 4. SQS ë©”ì‹œì§€ ì‚­ì œ
            self.sqs.delete_message(QueueUrl=queue_url, ReceiptHandle=msg["ReceiptHandle"])
            
            logger.info("âœ… Task Completed", id=task.request_id, ms=result.duration_ms)

            # Metrics Update
            status = "success" if result.success else "failure"
            self.jobs_processed.labels(status=status, runtime=task.runtime).inc()
            self.job_duration.labels(runtime=task.runtime).observe(result.duration_ms / 1000.0)

        except Exception as e:
            logger.error("Task processing failed", error=str(e))
            self.jobs_processed.labels(status="error", runtime="unknown").inc()
            
        finally:
            self.active_jobs.dec()

if __name__ == "__main__":
    agent = NanoAgent()
    agent.run()